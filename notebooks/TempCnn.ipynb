{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b992b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# resnet50.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.layers import (Conv2D, BatchNormalization, Activation, \n",
    "                                    MaxPooling2D, Add, GlobalAveragePooling2D, \n",
    "                                    Flatten, Dense, Input)\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from utils.metrics import calculate_metrics, print_metrics_summary\n",
    "from utils.visualization import save_visualizations\n",
    "\n",
    "class ResNet50:\n",
    "    def __init__(self, input_shape, num_classes, model_dir=\"saved_models\"):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.model_dir = model_dir\n",
    "        self.log_dir = os.path.join(model_dir, \"logs\")\n",
    "        self.model = self._build_model()\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "    \n",
    "    def _residual_block(self, X_start, filters, name, reduce=False, res_conv2d=False):\n",
    "        nb_filters_1, nb_filters_2, nb_filters_3 = filters\n",
    "        strides_1 = [2,2] if reduce else [1,1]\n",
    "\n",
    "        X = Conv2D(filters=nb_filters_1, kernel_size=[1,1], strides=strides_1, \n",
    "                  padding='same', name=f'{name}_conv1')(X_start)\n",
    "        X = BatchNormalization(name=f'{name}_bn1')(X)\n",
    "        X = Activation('relu')(X)\n",
    "\n",
    "        X = Conv2D(filters=nb_filters_2, kernel_size=[3,3], strides=[1,1], \n",
    "                  padding='same', name=f'{name}_conv2')(X)\n",
    "        X = BatchNormalization(name=f'{name}_bn2')(X)\n",
    "        X = Activation('relu')(X)\n",
    "\n",
    "        X = Conv2D(filters=nb_filters_3, kernel_size=[1,1], strides=[1,1], \n",
    "                  padding='same', name=f'{name}_conv3')(X)\n",
    "        X = BatchNormalization(name=f'{name}_bn3')(X)\n",
    "\n",
    "        if res_conv2d:\n",
    "            X_res = Conv2D(filters=nb_filters_3, kernel_size=[1,1], strides=strides_1, \n",
    "                         padding='same', name=f'{name}_conv_res')(X_start)\n",
    "            X_res = BatchNormalization(name=f'{name}_bn_res')(X_res)\n",
    "        else:\n",
    "            X_res = X_start\n",
    "\n",
    "        X = Add(name=f'{name}_add')([X, X_res])\n",
    "        return Activation('relu', name=f'{name}_relu')(X)\n",
    "\n",
    "    def _build_model(self):\n",
    "        X_input = Input(shape=self.input_shape, name='input')\n",
    "        # Architecture to be completed by user\n",
    "        outputs = X_input  # placeholder\n",
    "        return Model(inputs=X_input, outputs=outputs, name='ResNet50')\n",
    "\n",
    "    def _get_callbacks(self, model_name):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        checkpoint_path = os.path.join(self.model_dir, f\"{model_name}_best_{timestamp}.h5\")\n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "        tb_log_dir = os.path.join(self.log_dir, model_name, timestamp)\n",
    "        tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=tb_log_dir,\n",
    "            histogram_freq=1,\n",
    "            update_freq='epoch'\n",
    "        )\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            verbose=1\n",
    "        )\n",
    "        return [checkpoint, tensorboard, early_stop, lr_scheduler]\n",
    "\n",
    "    def train(self, train_x, train_y, valid_x, valid_y, \n",
    "              batch_size=32, epochs=100, learning_rate=1e-4, \n",
    "              weight_decay=1e-4, model_name=\"ResNet50\"):\n",
    "        self.encoder = LabelEncoder()\n",
    "        train_y_enc = self.encoder.fit_transform(train_y)\n",
    "        valid_y_enc = self.encoder.transform(valid_y)\n",
    "        callbacks = self._get_callbacks(model_name)\n",
    "        optimizer = tf.keras.optimizers.AdamW(\n",
    "            learning_rate=learning_rate,\n",
    "            weight_decay=weight_decay\n",
    "        )\n",
    "        self.model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"]\n",
    "        )\n",
    "        history = self.model.fit(\n",
    "            x=train_x,\n",
    "            y=train_y_enc,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(valid_x, valid_y_enc),\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        self.save_model(model_name)\n",
    "        return history\n",
    "\n",
    "    def save_model(self, model_name, save_format='h5'):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        if save_format == 'h5':\n",
    "            save_path = os.path.join(self.model_dir, f\"{model_name}_final_{timestamp}.h5\")\n",
    "            self.model.save(save_path)\n",
    "        else:\n",
    "            save_path = os.path.join(self.model_dir, f\"{model_name}_final_{timestamp}\")\n",
    "            self.model.save(save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "        return save_path\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        if not os.path.exists(model_path):\n",
    "            raise FileNotFoundError(f\"No model found at {model_path}\")\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        if self.model.input_shape[1:] != self.input_shape:\n",
    "            print(f\"Warning: Loaded model input shape {self.model.input_shape[1:]} \"\n",
    "                  f\"doesn't match expected {self.input_shape}\")\n",
    "        if self.model.output_shape[-1] != self.num_classes:\n",
    "            print(f\"Warning: Loaded model output shape {self.model.output_shape[-1]} \"\n",
    "                  f\"doesn't match expected {self.num_classes}\")\n",
    "        print(f\"Successfully loaded model from {model_path}\")\n",
    "        return self.model\n",
    "\n",
    "    def evaluate(self, test_x, test_y, model_name=\"ResNet50\", class_names=None):\n",
    "        test_y_enc = self.encoder.transform(test_y)\n",
    "        y_pred = np.argmax(self.model.predict(test_x), axis=1)\n",
    "        metrics = calculate_metrics(test_y_enc, y_pred, model_name)\n",
    "        print_metrics_summary(metrics)\n",
    "        save_visualizations(\n",
    "            model=self.model,\n",
    "            x_data=test_x,\n",
    "            y_true=test_y_enc,\n",
    "            y_pred=y_pred,\n",
    "            model_name=model_name,\n",
    "            class_names=class_names\n",
    "        )\n",
    "        self._save_evaluation_metrics(metrics, model_name)\n",
    "        return metrics\n",
    "\n",
    "    def _save_evaluation_metrics(self, metrics, model_name):\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        metrics_path = os.path.join(self.model_dir, f\"{model_name}_metrics_{timestamp}.txt\")\n",
    "        with open(metrics_path, 'w') as f:\n",
    "            for key, value in metrics.items():\n",
    "                if isinstance(value, (np.ndarray, list)):\n",
    "                    f.write(f\"{key}: {[round(v, 4) for v in value]}\n",
    "\")\n",
    "                else:\n",
    "                    f.write(f\"{key}: {round(value, 4)}\n",
    "\")\n",
    "        print(f\"Metrics saved to {metrics_path}\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
