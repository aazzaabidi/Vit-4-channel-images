# %% [markdown]
"""
# Random Forest Classifier Notebook
**Framework-Compatible Implementation**

This notebook provides an end-to-end workflow for training and evaluating a Random Forest classifier within your existing framework.
"""

# %% [markdown]
"""
## 1. Setup
First, let's import all required dependencies and configure the environment.
"""

# %%
import numpy as np
import os
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, PredefinedSplit
import matplotlib.pyplot as plt

# Import your framework utilities
try:
    from utils.metrics import calculate_metrics, print_metrics_summary
    from utils.visualization import save_visualizations
except ImportError:
    print("Framework utilities not found - running in standalone mode")
    # Fallback implementations would go here

# %%
# Configuration
MODEL_NAME = "RandomForest"
SAVE_DIR = "saved_models"
os.makedirs(SAVE_DIR, exist_ok=True)

# %% [markdown]
"""
## 2. Data Preparation
Load your dataset using the framework's data loader or custom implementation.
"""

# %%
def load_data():
    """Example data loading function - replace with your actual implementation"""
    print("Loading data...")
    # Mock data - replace with real data loading
    train_X = np.random.rand(1000, 23, 4)
    train_y = np.random.randint(0, 7, 1000)
    valid_X = np.random.rand(200, 23, 4)
    valid_y = np.random.randint(0, 7, 200)
    test_X = np.random.rand(300, 23, 4)
    test_y = np.random.randint(0, 7, 300)
    return (train_X, train_y, valid_X, valid_y, test_X, test_y)

# Load dataset
train_X, train_y, valid_X, valid_y, test_X, test_y = load_data()

# %% [markdown]
"""
## 3. Model Training
Hyperparameter tuning with GridSearchCV using predefined train/validation split.
"""

# %%
def train_random_forest(train_X, valid_X, train_y, valid_y):
    """Core training function with hyperparameter tuning"""
    
    # Hyperparameter grid
    tuned_parameters = {
        'n_estimators': [50, 100, 200],
        'max_depth': [5, 10, None],
        'min_samples_split': [2, 5],
        'class_weight': ['balanced', None]
    }
    
    # Prepare combined data for PredefinedSplit
    X = np.concatenate((train_X.reshape(train_X.shape[0], -1), 
                        valid_X.reshape(valid_X.shape[0], -1)), axis=0)
    y = np.concatenate((train_y, valid_y), axis=0)
    test_fold = np.concatenate([
        np.full(train_y.shape[0], -1),  # Training indices
        np.zeros(valid_y.shape[0])      # Validation indices
    ])
    
    # Initialize GridSearch
    clf = GridSearchCV(
        estimator=RandomForestClassifier(random_state=42),
        param_grid=tuned_parameters,
        cv=PredefinedSplit(test_fold),
        n_jobs=-1,
        verbose=2,
        scoring='f1_weighted'
    )
    
    # Train model
    print("\nStarting GridSearch...")
    clf.fit(X, y)
    
    return clf.best_estimator_, clf.best_params_

# %%
# Execute training
best_model, best_params = train_random_forest(train_X, valid_X, train_y, valid_y)

# %% [markdown]
"""
## 4. Evaluation
Model evaluation using framework metrics and visualization functions.
"""

# %%
def evaluate_model(model, test_X, test_y, model_name=MODEL_NAME):
    """Comprehensive model evaluation"""
    
    # Prepare test data
    test_X_flat = test_X.reshape(test_X.shape[0], -1)
    
    # Predictions
    test_pred = model.predict(test_X_flat)
    
    # Calculate metrics
    metrics = calculate_metrics(test_y, test_pred, model_name)
    
    # Print results
    print("\n=== EVALUATION RESULTS ===")
    print_metrics_summary(metrics)
    
    # Visualizations
    save_visualizations(
        model=model,
        x_data=test_X_flat,
        y_true=test_y,
        y_pred=test_pred,
        model_name=model_name
    )
    
    return metrics

# %%
# Run evaluation
metrics = evaluate_model(best_model, test_X, test_y)

# %% [markdown]
"""
## 5. Model Persistence
Save the trained model and metrics for future use.
"""

# %%
def save_artifacts(model, metrics, best_params, model_name=MODEL_NAME, save_dir=SAVE_DIR):
    """Save all training artifacts"""
    
    # Save model
    model_path = os.path.join(save_dir, f"{model_name}.joblib")
    joblib.dump(model, model_path)
    print(f"\nModel saved to {model_path}")
    
    # Save metrics
    metrics_path = os.path.join(save_dir, f"{model_name}_metrics.txt")
    with open(metrics_path, 'w') as f:
        f.write(f"Best Parameters:\n{best_params}\n\n")
        f.write("Evaluation Metrics:\n")
        for k, v in metrics.items():
            f.write(f"{k}: {v}\n")
    print(f"Metrics saved to {metrics_path}")

# %%
# Save everything
save_artifacts(best_model, metrics, best_params)

# %% [markdown]
"""
## 6. Loading Saved Models
Example of how to load and use a saved model.
"""

# %%
def load_saved_model(model_path):
    """Load a persisted model"""
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"No model found at {model_path}")
    return joblib.load(model_path)

# Example usage:
# loaded_model = load_saved_model("saved_models/RandomForest.joblib")
# new_predictions = loaded_model.predict(new_data)
