
````markdown
# ViT-MAE Model Implementation

This notebook implements the Vision Transformer Masked Autoencoder (ViT-MAE) model. It is a self-supervised learning model for learning useful representations from image patches, with an added classification head for downstream tasks.

## Setup
We will start by importing the necessary libraries and defining the ViT-MAE model class.

```python
# models/vit_mae.py
import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np
import os
from utils.metrics import calculate_metrics, print_metrics_summary
from utils.visualization import save_visualizations
````

## ViT-MAE Model Class

We define the `ViT_MAE` class that implements the encoder-decoder architecture for Masked Autoencoders.

```python
class ViT_MAE:
    def __init__(self, input_shape=(23, 4), num_classes=7, model_dir="saved_models/vit_mae", mask_ratio=0.75):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model_dir = model_dir
        self.mask_ratio = mask_ratio
        os.makedirs(self.model_dir, exist_ok=True)
        
        # Build MAE components
        self.encoder, self.decoder = self._build_mae()
        self.classifier = self._add_classification_head()
        
    def _build_mae(self):
        """Build MAE encoder-decoder architecture"""
        # Encoder
        inputs = layers.Input(shape=self.input_shape)
        patches = Patches(patch_size=4)(inputs)  # (None, 6, 16)
        
        # Masking
        self.mask_token = tf.Variable(tf.random.normal([1, 1, 64]), trainable=True)
        masked_patches, _ = self._random_masking(patches)
        
        # Positional embeddings
        num_patches = (self.input_shape[0] // 4) * (self.input_shape[1] // 1)
        positions = tf.range(start=0, limit=num_patches, delta=1)
        pos_embed = layers.Embedding(input_dim=num_patches, output_dim=64)(positions)
        
        # Encoder processing
        x = layers.Dense(64)(masked_patches) + pos_embed
        for _ in range(6):  # 6 transformer blocks
            x = layers.LayerNormalization(epsilon=1e-6)(x)
            x = layers.MultiHeadAttention(num_heads=4, key_dim=16)(x, x)
            x = layers.LayerNormalization(epsilon=1e-6)(x)
            x = layers.Dense(128, activation='gelu')(x)
            x = layers.Dense(64)(x)
        encoder_output = x
        
        # Decoder
        decoder_inputs = layers.Input(shape=(None, 64))
        x = layers.Dense(128)(decoder_inputs) + pos_embed
        for _ in range(2):  # Shallow decoder
            x = layers.LayerNormalization(epsilon=1e-6)(x)
            x = layers.MultiHeadAttention(num_heads=2, key_dim=16)(x, x)
            x = layers.Dense(128, activation='gelu')(x)
        reconstructed = layers.Dense(16)(x)  # Reconstruct original patch size
        
        return Model(inputs, encoder_output), Model(decoder_inputs, reconstructed)
    
    def _add_classification_head(self, trainable=True):
        """Add classification head with linear probing option"""
        inputs = layers.Input(shape=(None, 64))
        
        # Linear probe (non-trainable backbone) vs fine-tuning (trainable)
        x = layers.GlobalAveragePooling1D()(inputs)
        if not trainable:  # Linear probing
            x = layers.Lambda(lambda x: tf.stop_gradient(x))(x)
        
        outputs = layers.Dense(self.num_classes, activation='softmax')(x)
        return Model(inputs, outputs)
```

## Training

We can train the ViT-MAE model using different modes: 'pretrain', 'fine-tune', or 'linear-probe'. The training function will automatically switch between these modes.

```python
    def train(self, x_train, y_train, x_val, y_val, 
             mode='fine-tune', epochs=50, batch_size=64):
        """
        Training interface with mode selection:
        - 'pretrain': Self-supervised MAE pretraining
        - 'fine-tune': Full fine-tuning (default)
        - 'linear-probe': Linear probing (frozen encoder)
        """
        if mode == 'pretrain':
            return self.pretrain(x_train, epochs=epochs, batch_size=batch_size)
        else:
            # Rebuild classifier for selected mode
            self.classifier = self._add_classification_head(
                trainable=(mode == 'fine-tune')
            )
            return self.finetune(
                x_train, y_train, 
                x_val, y_val,
                epochs=epochs,
                batch_size=batch_size
            )
```

## Pretraining and Fine-Tuning

Here, we define the pretraining and fine-tuning methods for self-supervised and supervised training.

```python
    def pretrain(self, x_train, epochs=100, batch_size=256):
        """Self-supervised pretraining"""
        # Custom MAE loss (only masked patches)
        def mae_loss(y_true, y_pred):
            mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)
            return tf.reduce_mean(tf.square(y_true - y_pred) * mask)
        
        # Compile encoder-decoder
        mae_model = Model(
            inputs=self.encoder.input,
            outputs=self.decoder(self.encoder.output)
        )
        mae_model.compile(optimizer=tf.keras.optimizers.AdamW(1e-4), loss=mae_loss)
        
        # Train to reconstruct inputs
        history = mae_model.fit(
            x_train, x_train,
            batch_size=batch_size,
            epochs=epochs,
            callbacks=[
                tf.keras.callbacks.ModelCheckpoint(
                    os.path.join(self.model_dir, "pretrained_encoder.h5"),
                    save_best_only=True,
                    save_weights_only=True
                )
            ]
        )
        return history
```

## Evaluation

The model can be evaluated using standard metrics.

```python
    def evaluate(self, x_test, y_test):
        """Unified evaluation for both modes"""
        y_test_enc = tf.keras.utils.to_categorical(y_test, self.num_classes)
        model = Model(
            inputs=self.encoder.input,
            outputs=self.classifier(self.encoder.output)
        )
        
        results = model.evaluate(x_test, y_test_enc, verbose=0)
        y_pred = np.argmax(model.predict(x_test), axis=1)
        
        metrics = {
            'loss': results[0],
            'accuracy': results[1],
            **calculate_metrics(y_test, y_pred, "ViT-MAE")
        }
        print_metrics_summary(metrics)
        
        save_visualizations(
            model=model,
            x_data=x_test,
            y_true=y_test,
            y_pred=y_pred,
            model_name="ViT-MAE"
        )
        return metrics
```

## Example Usage

Finally, we provide an example of how to use the model for pretraining, fine-tuning, and evaluation.

```python
# Usage Example:
if __name__ == "__main__":
    # Initialize
    vit_mae = ViT_MAE(input_shape=(23, 4), num_classes=7)
    
    # Option 1: Full pretraining + fine-tuning
    vit_mae.train(x_train, y_train, x_val, y_val, mode='pretrain', epochs=100)
    vit_mae.train(x_train, y_train, x_val, y_val, mode='fine-tune', epochs=50)
    
    # Option 2: Linear probing only (frozen encoder)
    vit_mae.train(x_train, y_train, x_val, y_val, mode='linear-probe', epochs=50)
    
    # Evaluate
    metrics = vit_mae.evaluate(x_test, y_test)
```

