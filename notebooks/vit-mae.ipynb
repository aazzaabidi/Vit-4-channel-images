
```python
# Cell 1: Imports
import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np
import os


```

```python
# Cell 2: Patch Layer Definition (if not provided elsewhere)
class Patches(layers.Layer):
    def __init__(self, patch_size):
        super(Patches, self).__init__()
        self.patch_size = patch_size

    def call(self, images):
        batch_size = tf.shape(images)[0]
        patches = tf.image.extract_patches(
            images=tf.expand_dims(images, -1),  # Add channel dimension if missing
            sizes=[1, self.patch_size, 1, 1],
            strides=[1, self.patch_size, 1, 1],
            rates=[1, 1, 1, 1],
            padding='VALID'
        )
        patch_dims = patches.shape[-1]
        patches = tf.reshape(patches, [batch_size, -1, patch_dims])
        return patches
```

```python
# Cell 3: ViT-MAE class
class ViT_MAE:
    def __init__(self, input_shape=(23, 4), num_classes=7, model_dir="saved_models/vit_mae", mask_ratio=0.75):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model_dir = model_dir
        self.mask_ratio = mask_ratio
        os.makedirs(self.model_dir, exist_ok=True)
        
        self.encoder, self.decoder = self._build_mae()
        self.classifier = self._add_classification_head()
        
    def _random_masking(self, patches):
        num_patches = tf.shape(patches)[1]
        len_keep = tf.cast(num_patches * (1 - self.mask_ratio), tf.int32)
        
        noise = tf.random.uniform(shape=(tf.shape(patches)[0], num_patches))
        ids_shuffle = tf.argsort(noise, axis=1)
        ids_restore = tf.argsort(ids_shuffle, axis=1)
        
        ids_keep = ids_shuffle[:, :len_keep]
        batch_indices = tf.tile(tf.range(tf.shape(patches)[0])[:, None], [1, len_keep])
        gather_nd_indices = tf.stack([batch_indices, ids_keep], axis=-1)
        masked = tf.gather_nd(patches, gather_nd_indices)
        
        return masked, ids_restore

    def _build_mae(self):
        inputs = layers.Input(shape=self.input_shape)
        patches = Patches(patch_size=4)(inputs)

        self.mask_token = tf.Variable(tf.random.normal([1, 1, 64]), trainable=True)
        masked_patches, _ = self._random_masking(patches)
        
        num_patches = tf.shape(patches)[1]
        pos_embed = layers.Embedding(input_dim=num_patches, output_dim=64)(tf.range(num_patches))
        
        x = layers.Dense(64)(masked_patches) + pos_embed
        for _ in range(6):
            x = layers.LayerNormalization(epsilon=1e-6)(x)
            x = layers.MultiHeadAttention(num_heads=4, key_dim=16)(x, x)
            x = layers.LayerNormalization(epsilon=1e-6)(x)
            x = layers.Dense(128, activation='gelu')(x)
            x = layers.Dense(64)(x)
        encoder_output = x

        decoder_inputs = layers.Input(shape=(None, 64))
        x = layers.Dense(128)(decoder_inputs) + pos_embed
        for _ in range(2):
            x = layers.LayerNormalization(epsilon=1e-6)(x)
            x = layers.MultiHeadAttention(num_heads=2, key_dim=16)(x, x)
            x = layers.Dense(128, activation='gelu')(x)
        reconstructed = layers.Dense(16)(x)

        return Model(inputs, encoder_output), Model(decoder_inputs, reconstructed)
    
    def _add_classification_head(self, trainable=True):
        inputs = layers.Input(shape=(None, 64))
        x = layers.GlobalAveragePooling1D()(inputs)
        if not trainable:
            x = layers.Lambda(lambda x: tf.stop_gradient(x))(x)
        outputs = layers.Dense(self.num_classes, activation='softmax')(x)
        return Model(inputs, outputs)
    
    def train(self, x_train, y_train, x_val, y_val, mode='fine-tune', epochs=50, batch_size=64):
        if mode == 'pretrain':
            return self.pretrain(x_train, epochs=epochs, batch_size=batch_size)
        else:
            self.classifier = self._add_classification_head(trainable=(mode == 'fine-tune'))
            return self.finetune(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size)
    
    def pretrain(self, x_train, epochs=100, batch_size=256):
        def mae_loss(y_true, y_pred):
            mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)
            return tf.reduce_mean(tf.square(y_true - y_pred) * mask)
        
        mae_model = Model(inputs=self.encoder.input, outputs=self.decoder(self.encoder.output))
        mae_model.compile(optimizer=tf.keras.optimizers.AdamW(1e-4), loss=mae_loss)
        
        history = mae_model.fit(
            x_train, x_train,
            batch_size=batch_size,
            epochs=epochs,
            callbacks=[
                tf.keras.callbacks.ModelCheckpoint(
                    os.path.join(self.model_dir, "pretrained_encoder.h5"),
                    save_best_only=True,
                    save_weights_only=True
                )
            ]
        )
        return history
    
    def finetune(self, x_train, y_train, x_val, y_val, epochs=50, batch_size=64):
        self.encoder.trainable = not isinstance(self.classifier.layers[1], layers.Lambda)
        
        y_train_enc = tf.keras.utils.to_categorical(y_train, self.num_classes)
        y_val_enc = tf.keras.utils.to_categorical(y_val, self.num_classes)
        
        model = Model(inputs=self.encoder.input, outputs=self.classifier(self.encoder.output))
        model.compile(optimizer=tf.keras.optimizers.AdamW(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])
        
        history = model.fit(
            x_train, y_train_enc,
            validation_data=(x_val, y_val_enc),
            batch_size=batch_size,
            epochs=epochs,
            callbacks=[
                tf.keras.callbacks.ModelCheckpoint(
                    os.path.join(self.model_dir, f"{'linear' if not self.encoder.trainable else 'finetune'}_model.h5"),
                    save_best_only=True
                )
            ]
        )
        return history
    
    def evaluate(self, x_test, y_test):
        y_test_enc = tf.keras.utils.to_categorical(y_test, self.num_classes)
        model = Model(inputs=self.encoder.input, outputs=self.classifier(self.encoder.output))
        results = model.evaluate(x_test, y_test_enc, verbose=0)
        y_pred = np.argmax(model.predict(x_test), axis=1)
        
        metrics = {
            'loss': results[0],
            'accuracy': results[1],
            **calculate_metrics(y_test, y_pred, "ViT-MAE")
        }
        print_metrics_summary(metrics)
        
        save_visualizations(model=model, x_data=x_test, y_true=y_test, y_pred=y_pred, model_name="ViT-MAE")
        return metrics
```

```python
# Cell 4: Usage Example
# NOTE: You must define x_train, y_train, x_val, y_val, x_test, y_test before running this
vit_mae = ViT_MAE(input_shape=(23, 4), num_classes=7)

# Pretraining
vit_mae.train(x_train, y_train, x_val, y_val, mode='pretrain', epochs=100)

# Fine-tuning
vit_mae.train(x_train, y_train, x_val, y_val, mode='fine-tune', epochs=50)

# Or linear probing
vit_mae.train(x_train, y_train, x_val, y_val, mode='linear-probe', epochs=50)

# Evaluation
metrics = vit_mae.evaluate(x_test, y_test)
```

