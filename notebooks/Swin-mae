import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np
import os
from utils.metrics import calculate_metrics, print_metrics_summary
from utils.visualization import save_visualizations

# Custom simplified Swin Block for time series
class SwinBlock(layers.Layer):
    def __init__(self, dim, num_heads, mlp_ratio=4., dropout=0.1):
        super(SwinBlock, self).__init__()
        self.norm1 = layers.LayerNormalization()
        self.attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=dim // num_heads, dropout=dropout)
        self.norm2 = layers.LayerNormalization()
        self.mlp = tf.keras.Sequential([
            layers.Dense(dim * mlp_ratio, activation='gelu'),
            layers.Dropout(dropout),
            layers.Dense(dim)
        ])

    def call(self, x):
        x = x + self.attn(self.norm1(x), self.norm1(x))
        x = x + self.mlp(self.norm2(x))
        return x

class Swin_MAE:
    def __init__(self, input_shape=(23, 4), num_classes=7, model_dir="saved_models/swin_mae", mask_ratio=0.75):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model_dir = model_dir
        self.mask_ratio = mask_ratio
        os.makedirs(self.model_dir, exist_ok=True)

        self.encoder, self.decoder = self._build_mae()
        self.classifier = self._add_classification_head()

    def _random_masking(self, patches):
        """Random masking of patches"""
        B, L, D = tf.shape(patches)[0], tf.shape(patches)[1], tf.shape(patches)[2]
        len_keep = tf.cast(tf.cast(L, tf.float32) * (1 - self.mask_ratio), tf.int32)
        noise = tf.random.uniform((B, L))
        ids_shuffle = tf.argsort(noise, axis=1)
        ids_restore = tf.argsort(ids_shuffle, axis=1)
        ids_keep = ids_shuffle[:, :len_keep]
        batch_range = tf.reshape(tf.range(B)[:, None], (-1, 1))
        x_masked = tf.gather_nd(patches, tf.concat([batch_range, tf.expand_dims(ids_keep, -1)], axis=-1), batch_dims=1)
        return x_masked, ids_restore

    def _build_mae(self):
        inputs = layers.Input(shape=self.input_shape)  # (23, 4)

        # Patch embedding (project each timestep)
        x = layers.Dense(64)(inputs)  # (23, 64)

        # Apply random masking
        self.mask_token = tf.Variable(tf.random.normal([1, 1, 64]), trainable=True)
        masked_x, _ = self._random_masking(x)

        # Positional encoding
        pos_embed = layers.Embedding(input_dim=self.input_shape[0], output_dim=64)(tf.range(self.input_shape[0]))
        masked_x += pos_embed[:tf.shape(masked_x)[1]]

        # Swin-like encoder: stacked Swin blocks
        for _ in range(4):
            masked_x = SwinBlock(dim=64, num_heads=4)(masked_x)

        encoder_output = masked_x

        # Decoder
        decoder_inputs = layers.Input(shape=(None, 64))
        x = decoder_inputs + pos_embed[:tf.shape(decoder_inputs)[1]]
        for _ in range(2):
            x = SwinBlock(dim=64, num_heads=2)(x)
        reconstructed = layers.Dense(self.input_shape[-1])(x)

        return Model(inputs, encoder_output), Model(decoder_inputs, reconstructed)

    def _add_classification_head(self, trainable=True):
        inputs = layers.Input(shape=(None, 64))
        x = layers.GlobalAveragePooling1D()(inputs)
        if not trainable:
            x = layers.Lambda(lambda x: tf.stop_gradient(x))(x)
        outputs = layers.Dense(self.num_classes, activation='softmax')(x)
        return Model(inputs, outputs)

    def train(self, x_train, y_train, x_val, y_val, mode='fine-tune', epochs=50, batch_size=64):
        if mode == 'pretrain':
            return self.pretrain(x_train, epochs=epochs, batch_size=batch_size)
        else:
            self.classifier = self._add_classification_head(trainable=(mode == 'fine-tune'))
            return self.finetune(x_train, y_train, x_val, y_val, epochs=epochs, batch_size=batch_size)

    def pretrain(self, x_train, epochs=100, batch_size=256):
        def mae_loss(y_true, y_pred):
            mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)
            return tf.reduce_mean(tf.square(y_true - y_pred) * mask)

        mae_model = Model(inputs=self.encoder.input, outputs=self.decoder(self.encoder.output))
        mae_model.compile(optimizer=tf.keras.optimizers.AdamW(1e-4), loss=mae_loss)

        history = mae_model.fit(
            x_train, x_train,
            batch_size=batch_size,
            epochs=epochs,
            callbacks=[
                tf.keras.callbacks.ModelCheckpoint(
                    os.path.join(self.model_dir, "pretrained_encoder.h5"),
                    save_best_only=True,
                    save_weights_only=True
                )
            ]
        )
        return history

    def finetune(self, x_train, y_train, x_val, y_val, epochs=50, batch_size=64):
        self.encoder.trainable = not isinstance(self.classifier.layers[1], layers.Lambda)

        y_train_enc = tf.keras.utils.to_categorical(y_train, self.num_classes)
        y_val_enc = tf.keras.utils.to_categorical(y_val, self.num_classes)

        model = Model(inputs=self.encoder.input, outputs=self.classifier(self.encoder.output))
        model.compile(optimizer=tf.keras.optimizers.AdamW(1e-5), loss='categorical_crossentropy', metrics=['accuracy'])

        history = model.fit(
            x_train, y_train_enc,
            validation_data=(x_val, y_val_enc),
            batch_size=batch_size,
            epochs=epochs,
            callbacks=[
                tf.keras.callbacks.ModelCheckpoint(
                    os.path.join(self.model_dir, f"{'linear' if not self.encoder.trainable else 'finetune'}_model.h5"),
                    save_best_only=True
                )
            ]
        )
        return history

    def evaluate(self, x_test, y_test):
        y_test_enc = tf.keras.utils.to_categorical(y_test, self.num_classes)
        model = Model(inputs=self.encoder.input, outputs=self.classifier(self.encoder.output))

        results = model.evaluate(x_test, y_test_enc, verbose=0)
        y_pred = np.argmax(model.predict(x_test), axis=1)

        metrics = {
            'loss': results[0],
            'accuracy': results[1],
            **calculate_metrics(y_test, y_pred, "Swin-MAE")
        }
        print_metrics_summary(metrics)

        save_visualizations(
            model=model,
            x_data=x_test,
            y_true=y_test,
            y_pred=y_pred,
            model_name="Swin-MAE"
        )
        return metrics
