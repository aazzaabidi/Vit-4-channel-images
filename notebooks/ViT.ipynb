{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Vision Transformer (ViT) Implementation in TensorFlow\nThis notebook provides a modular and clear implementation of a Vision Transformer (ViT) adapted for time-series data."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import tensorflow as tf\nfrom tensorflow.keras import layers, Model\nimport numpy as np\nimport os\nfrom utils.metrics import calculate_metrics, print_metrics_summary\nfrom utils.visualization import save_visualizations\nfrom sklearn.preprocessing import LabelEncoder"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Patches Layer\nThis layer splits the input into non-overlapping patches."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class Patches(layers.Layer):\n    def __init__(self, patch_size):\n        super().__init__()\n        self.patch_size = patch_size\n\n    def call(self, x):\n        batch_size = tf.shape(x)[0]\n        patches = tf.reshape(x, [batch_size, -1, self.patch_size * x.shape[-1]])\n        return patches"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Patch Encoder\nThis layer encodes each patch using a linear projection and adds positional embedding."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class PatchEncoder(layers.Layer):\n    def __init__(self, num_patches, projection_dim):\n        super().__init__()\n        self.num_patches = num_patches\n        self.projection = layers.Dense(projection_dim)\n        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n\n    def call(self, patch):\n        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n        encoded = self.projection(patch) + self.position_embedding(positions)\n        return encoded"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Vision Transformer (ViT) Class\nMain model class defining the Vision Transformer architecture, training, and evaluation methods."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class ViT:\n    def __init__(self, input_shape=(23, 4), num_classes=7, model_dir=\"saved_models/vit\"):\n        self.input_shape = input_shape\n        self.num_classes = num_classes\n        self.model_dir = model_dir\n        os.makedirs(self.model_dir, exist_ok=True)\n        self.model = self._build_model()\n        self.encoder = LabelEncoder()\n\n    def _mlp(self, x, hidden_units, dropout_rate):\n        for units in hidden_units:\n            x = layers.Dense(units, activation=tf.nn.gelu)(x)\n            x = layers.Dropout(dropout_rate)(x)\n        return x\n\n    def _build_model(self):\n        inputs = layers.Input(shape=self.input_shape)\n        patches = Patches(patch_size=4)(inputs)\n        num_patches = (self.input_shape[0] // 4) * (self.input_shape[1] // 1)\n        encoded_patches = PatchEncoder(num_patches, projection_dim=64)(patches)\n\n        for _ in range(6):\n            x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n            attention_output = layers.MultiHeadAttention(num_heads=4, key_dim=64, dropout=0.1)(x1, x1)\n            x2 = layers.Add()([attention_output, encoded_patches])\n            x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n            x3 = self._mlp(x3, hidden_units=[128, 64], dropout_rate=0.1)\n            encoded_patches = layers.Add()([x3, x2])\n\n        representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        representation = layers.Flatten()(representation)\n        features = self._mlp(representation, hidden_units=[128, 64], dropout_rate=0.5)\n        logits = layers.Dense(self.num_classes)(features)\n        return Model(inputs=inputs, outputs=logits)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Training, Evaluation and Model I/O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "    def train(self, x_train, y_train, x_val, y_val, epochs=100, batch_size=64):\n        y_train_enc = self.encoder.fit_transform(y_train)\n        y_val_enc = self.encoder.transform(y_val)\n\n        callbacks = [\n            tf.keras.callbacks.ModelCheckpoint(\n                os.path.join(self.model_dir, \"best_model.h5\"), monitor=\"val_accuracy\", save_best_only=True\n            ),\n            tf.keras.callbacks.EarlyStopping(\n                monitor=\"val_accuracy\", patience=10, restore_best_weights=True\n            )\n        ]\n\n        self.model.compile(\n            optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-3),\n            loss=\"sparse_categorical_crossentropy\",\n            metrics=[\"accuracy\"]\n        )\n\n        history = self.model.fit(\n            x=x_train,\n            y=y_train_enc,\n            validation_data=(x_val, y_val_enc),\n            batch_size=batch_size,\n            epochs=epochs,\n            callbacks=callbacks\n        )\n        return history\n\n    def evaluate(self, x_test, y_test):\n        y_test_enc = self.encoder.transform(y_test)\n        y_pred = np.argmax(self.model.predict(x_test), axis=1)\n        metrics = calculate_metrics(y_test_enc, y_pred, \"ViT\")\n        print_metrics_summary(metrics)\n        save_visualizations(self.model, x_test, y_test_enc, y_pred, model_name=\"ViT\")\n        return metrics\n\n    def save(self, model_name=\"vit_model\"):\n        save_path = os.path.join(self.model_dir, f\"{model_name}.h5\")\n        self.model.save(save_path)\n        print(f\"Model saved to {save_path}\")\n\n    @classmethod\n    def load(cls, model_path):\n        model = tf.keras.models.load_model(model_path)\n        vit = cls(input_shape=model.input_shape[1:], num_classes=model.output_shape[-1])\n        vit.model = model\n        return vit"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
