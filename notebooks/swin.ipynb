# models/swin_transformer.py
import tensorflow as tf
from tensorflow.keras import layers, Model
import numpy as np
import os
from utils.metrics import calculate_metrics, print_metrics_summary
from utils.visualization import save_visualizations

class SwinTransformer:
    def __init__(self, input_shape=(23, 4), num_classes=7, model_dir="saved_models/swin"):
        self.input_shape = input_shape
        self.num_classes = num_classes
        self.model_dir = model_dir
        os.makedirs(self.model_dir, exist_ok=True)
        self.model = self._build_model()
        self.encoder = LabelEncoder()

    def _build_model(self):
        """Build Swin Transformer model for time series"""
        inputs = layers.Input(shape=self.input_shape)
        
        # 1. Patch Partition (Convert to 2D "images")
        x = layers.Reshape((self.input_shape[0], self.input_shape[1], 1))(inputs)
        
        # 2. Initial Patch Embedding
        x = layers.Conv2D(64, kernel_size=(4, 1), strides=(4, 1), padding='same')(x)
        x = layers.LayerNormalization(epsilon=1e-5)(x)
        
        # 3. Swin Transformer Blocks
        for _ in range(2):  # Two stages
            # Window attention
            x = SwinTransformerBlock(64, num_heads=4, window_size=7)(x)
            # Shifted window attention
            x = SwinTransformerBlock(64, num_heads=4, window_size=7, shift_size=3)(x)
            # Patch merging
            x = PatchMerging()(x)
        
        # 4. Classification Head
        x = layers.GlobalAveragePooling2D()(x)
        x = layers.Dense(128, activation='gelu')(x)
        outputs = layers.Dense(self.num_classes, activation='softmax')(x)
        
        return Model(inputs=inputs, outputs=outputs)

    def train(self, x_train, y_train, x_val, y_val, epochs=50, batch_size=64):
        """Training procedure"""
        # Encode labels
        y_train_enc = self.encoder.fit_transform(y_train)
        y_val_enc = self.encoder.transform(y_val)
        
        # Callbacks
        callbacks = [
            tf.keras.callbacks.ModelCheckpoint(
                os.path.join(self.model_dir, "best_model.h5"),
                monitor="val_accuracy",
                save_best_only=True
            ),
            tf.keras.callbacks.EarlyStopping(
                monitor="val_accuracy",
                patience=10,
                restore_best_weights=True
            )
        ]
        
        # Compile
        self.model.compile(
            optimizer=tf.keras.optimizers.AdamW(learning_rate=1e-4, weight_decay=1e-3),
            loss="sparse_categorical_crossentropy",
            metrics=["accuracy"]
        )
        
        # Train
        history = self.model.fit(
            x_train, y_train_enc,
            validation_data=(x_val, y_val_enc),
            batch_size=batch_size,
            epochs=epochs,
            callbacks=callbacks
        )
        return history

    def evaluate(self, x_test, y_test):
        """Evaluation with metrics and visualizations"""
        y_test_enc = self.encoder.transform(y_test)
        y_pred = np.argmax(self.model.predict(x_test), axis=1)
        
        metrics = calculate_metrics(y_test_enc, y_pred, "SwinTransformer")
        print_metrics_summary(metrics)
        
        save_visualizations(
            model=self.model,
            x_data=x_test,
            y_true=y_test_enc,
            y_pred=y_pred,
            model_name="SwinTransformer"
        )
        return metrics

    def save(self, model_name="swin_model"):
        save_path = os.path.join(self.model_dir, f"{model_name}.h5")
        self.model.save(save_path)
        print(f"Model saved to {save_path}")

    @classmethod
    def load(cls, model_path):
        model = tf.keras.models.load_model(model_path, custom_objects={
            'SwinTransformerBlock': SwinTransformerBlock,
            'PatchMerging': PatchMerging
        })
        swin = cls(input_shape=model.input_shape[1:-1], 
                  num_classes=model.output_shape[-1])
        swin.model = model
        return swin

# Swin-specific components
class WindowAttention(layers.Layer):
    def __init__(self, dim, num_heads, window_size):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.window_size = window_size
        self.scale = (dim // num_heads) ** -0.5
        
    def build(self, input_shape):
        self.qkv = layers.Dense(self.dim * 3)
        self.proj = layers.Dense(self.dim)
        
    def call(self, x):
        B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]
        
        # Partition into windows
        x = tf.reshape(x, [B, H//self.window_size, self.window_size, 
                          W//self.window_size, self.window_size, C])
        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])
        x = tf.reshape(x, [-1, self.window_size*self.window_size, C])
        
        # Self-attention
        qkv = self.qkv(x)
        q, k, v = tf.split(qkv, 3, axis=-1)
        q = tf.reshape(q, [-1, self.window_size*self.window_size, self.num_heads, C//self.num_heads])
        q = tf.transpose(q, [0, 2, 1, 3])
        k = tf.reshape(k, [-1, self.window_size*self.window_size, self.num_heads, C//self.num_heads])
        k = tf.transpose(k, [0, 2, 1, 3])
        v = tf.reshape(v, [-1, self.window_size*self.window_size, self.num_heads, C//self.num_heads])
        v = tf.transpose(v, [0, 2, 1, 3])
        
        attn = (q @ tf.transpose(k, [0, 1, 3, 2])) * self.scale
        attn = tf.nn.softmax(attn, axis=-1)
        x = (attn @ v)
        x = tf.transpose(x, [0, 2, 1, 3])
        x = tf.reshape(x, [-1, self.window_size, self.window_size, C])
        
        # Merge windows
        x = tf.reshape(x, [B, H//self.window_size, W//self.window_size, 
                          self.window_size, self.window_size, C])
        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])
        x = tf.reshape(x, [B, H, W, C])
        
        return self.proj(x)

class SwinTransformerBlock(layers.Layer):
    def __init__(self, dim, num_heads, window_size, shift_size=0):
        super().__init__()
        self.dim = dim
        self.norm1 = layers.LayerNormalization(epsilon=1e-5)
        self.attn = WindowAttention(dim, num_heads, window_size)
        self.norm2 = layers.LayerNormalization(epsilon=1e-5)
        self.mlp = tf.keras.Sequential([
            layers.Dense(dim * 4, activation='gelu'),
            layers.Dense(dim)
        ])
        self.shift_size = shift_size
        self.window_size = window_size
        
    def call(self, x):
        H, W = tf.shape(x)[1], tf.shape(x)[2]
        
        # Shifted window attention
        if self.shift_size > 0:
            shifted_x = tf.roll(x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2])
        else:
            shifted_x = x
            
        # Window attention
        x = x + self.attn(self.norm1(shifted_x))
        x = x + self.mlp(self.norm2(x))
        return x

class PatchMerging(layers.Layer):
    def __init__(self):
        super().__init__()
        self.norm = layers.LayerNormalization(epsilon=1e-5)
        
    def call(self, x):
        B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]
        x = tf.reshape(x, [B, H//2, 2, W//2, 2, C])
        x = tf.transpose(x, [0, 1, 3, 2, 4, 5])
        x = tf.reshape(x, [B, H//2, W//2, 4*C])
        x = self.norm(x)
        x = layers.Dense(2*C)(x)  # Reduce channel dimension
        return x
